# üéÆ Video Game Big Data - An√°lisis con Hadoop, Spark y FastAPI

Sistema completo de an√°lisis de rese√±as de videojuegos utilizando tecnolog√≠as Big Data (HDFS, Spark) con una API REST para cEl an√°lisis completo generar√° **15 archivos CSV** con an√°lisis detallados.

**An√°lisis Incluidos:**

| #   | An√°lisis                              | Archivo CSV                    |
| --- | ------------------------------------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------- |
| 1   | Estad√≠sticas globales                 | `global_statistics.csv`        |
| 2   | Distribuci√≥n de ratings               | `rating_distribution.csv`      |
| 3   | Actividad por a√±o                     | `yearly_activity.csv`          |
| 4   | Actividad por mes                     | `monthly_activity.csv`         |
| 5   | Actividad por d√≠a de semana           | `day_of_week_analysis.csv`     |
| 6   | Top 1000 juegos m√°s rese√±ados         | `top_reviewed_games.csv`       |
| 7   | Top 1000 juegos mejor valorados       | `top_rated_games.csv`          |
| 8   | Top 1000 juegos peor valorados        | `worst_rated_games.csv`        |
| 9   | Longitud de texto vs rating           | `length_vs_rating.csv`         |
| 10  | Palabras frecuentes positivas         | `positive_words_frequency.csv` |
| 11  | Palabras frecuentes negativas         | `negative_words_frequency.csv` |
| 12  | Detecci√≥n de outliers                 | `rating_outliers.csv`          |
| 13  | Rese√±as verificadas vs no verificadas | `verified_statistics.csv`      |
| 14  | Rese√±as m√°s √∫tiles (helpful votes)    | `helpful_votes_analysis.csv`   |
| 15  | Top 1000 reviewers m√°s activos        | `top_reviewers.csv`            | s resultados. Incluye integraci√≥n con **Easyparser API** para obtener nombres de productos de Amazon en tiempo real. |

## üìã Requisitos Previos

- Docker Desktop instalado y ejecut√°ndose
- Git Bash o WSL (para Windows)
- Al menos 8GB de RAM disponible
- 10GB de espacio en disco
- **API Key de Easyparser** (para endpoints de productos con nombres)

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   DENTRO DE DOCKER (VOLUMEN INTERNO)                ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  1. HDFS (namenode) ‚Üí /videogames/Video_Games.json                 ‚îÇ
‚îÇ                           ‚Üì                                         ‚îÇ
‚îÇ  2. Spark procesa y genera 15 an√°lisis                             ‚îÇ
‚îÇ     ‚Üí shared-data:/data/results/*.csv (volumen interno)            ‚îÇ
‚îÇ                           ‚Üì                                         ‚îÇ
‚îÇ  3. API lee CSVs y consulta Easyparser API                         ‚îÇ
‚îÇ     ‚Üí shared-data:/data/results/*.csv + Easyparser                 ‚îÇ
‚îÇ     ‚Üí Cach√© de productos: /data/results/*.json                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ Expone puerto 8000
                            ‚ñº
                 http://localhost:8000/docs
                     (17 endpoints JSON)
```

### Componentes del Sistema

- **HDFS**: Sistema de archivos distribuido para almacenar el dataset (1.7GB)
- **Spark**: Motor de procesamiento Big Data para an√°lisis masivo de datos
- **FastAPI**: API REST con 17 endpoints para consultar resultados
- **Easyparser API**: Servicio externo para obtener nombres de productos de Amazon
- **Volumen Compartido**: Permite compartir datos entre Spark y la API

## üöÄ Gu√≠a de Ejecuci√≥n Paso a Paso

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/StalinAM/video-game-bigdata.git
cd video-game-bigdata
```

### Paso 2: Configurar la API Key de Easyparser

**‚ö†Ô∏è IMPORTANTE**: Antes de levantar los contenedores, debes configurar tu API Key de Easyparser.

**Ubicaci√≥n**: `api/amazon_scraper.py` (l√≠nea 15)

```python
# Configuraci√≥n de Easyparser
EASYPARSER_API_KEY = 'TU_API_KEY_AQUI'  # ‚Üê Reemplaza con tu API key
EASYPARSER_ENDPOINT = 'https://realtime.easyparser.com/v1/request'
```

**¬øD√≥nde obtener la API Key?**

- Reg√≠strate en [Easyparser](https://easyparser.com/)
- Crea una cuenta gratuita
- Copia tu API Key desde el dashboard
- P√©gala en el archivo `amazon_scraper.py`

**Nota**: Si no configuras la API Key, los endpoints de productos (`/products/*`) devolver√°n errores de autenticaci√≥n.

### Paso 3: Verificar la Estructura del Proyecto

```bash
ls -la
```

Deber√≠as ver:

```
docker-compose.yml
README.md
api/
  ‚îú‚îÄ‚îÄ amazon_scraper.py    # ‚Üê Configura tu API key aqu√≠
  ‚îú‚îÄ‚îÄ dockerfile
  ‚îú‚îÄ‚îÄ main.py
  ‚îî‚îÄ‚îÄ requirements.txt
data/
  ‚îî‚îÄ‚îÄ Video_Games.json
hadoop/
  ‚îú‚îÄ‚îÄ core-site.xml
  ‚îî‚îÄ‚îÄ hdfs-site.xml
spark/
  ‚îú‚îÄ‚îÄ comprehensive_analysis_simple.py
  ‚îî‚îÄ‚îÄ spark_analysis.py
```

### Paso 4: Levantar los Contenedores Docker

```bash
docker-compose up -d
```

Este comando crear√° y ejecutar√° **5 contenedores**:

| Contenedor     | Servicio      | Puerto     | Descripci√≥n                    |
| -------------- | ------------- | ---------- | ------------------------------ |
| `namenode`     | HDFS NameNode | 9870, 9000 | Nodo maestro de HDFS           |
| `datanode`     | HDFS DataNode | -          | Nodo de almacenamiento de HDFS |
| `spark-master` | Spark Master  | 8080, 7077 | Nodo maestro de Spark          |
| `spark-worker` | Spark Worker  | -          | Nodo trabajador de Spark       |
| `api`          | FastAPI       | 8000       | API REST                       |

**Verificar que los contenedores est√°n corriendo:**

```bash
docker ps
```

Deber√≠as ver los 5 contenedores con estado `Up`.

### Paso 5: Verificar que los Servicios est√°n Listos

Espera aproximadamente **30-60 segundos** para que todos los servicios inicien correctamente.

**Interfaces Web Disponibles:**

- üåê **HDFS NameNode**: http://localhost:9870
- üåê **Spark Master**: http://localhost:8080
- üåê **API Documentation**: http://localhost:8000/docs

**Verificar estado de HDFS:**

```bash
docker exec namenode hdfs dfsadmin -report
```

Deber√≠as ver informaci√≥n sobre el cluster y los datanodes conectados.

### Paso 6: Copiar el Dataset al Contenedor

```bash
docker cp ./data/Video_Games.json namenode:/tmp/Video_Games.json
```

Este comando copia el archivo JSON (1.7GB) al contenedor del namenode.

### Paso 7: Subir el Dataset a HDFS

```bash
docker exec -it namenode bash -c "hdfs dfs -mkdir -p /videogames && hdfs dfs -put -f /tmp/Video_Games.json /videogames/"
```

> **‚è≥ Nota**: Este proceso puede tomar **1-3 minutos** dependiendo de tu hardware. Ver√°s mensajes `INFO sasl.SaslDataTransferClient` que indican el progreso de la transferencia.

**Verificar que el archivo est√° en HDFS:**

```bash
docker exec namenode hdfs dfs -ls /videogames/
```

Salida esperada:

```
Found 1 items
-rw-r--r--   1 root supergroup 1702313074 2026-01-15 10:30 /videogames/Video_Games.json
```

### Paso 8: Ejecutar el An√°lisis de Spark

Tienes dos opciones de an√°lisis:

#### Opci√≥n A: An√°lisis B√°sico ‚ö° (R√°pido - ~1 minuto)

```bash
docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/spark_analysis.py
```

Genera un √∫nico archivo CSV con estad√≠sticas b√°sicas por juego.

#### Opci√≥n B: An√°lisis Completo üåü **RECOMENDADO** (~5-10 minutos)

```bash
# Git Bash / WSL
MSYS_NO_PATHCONV=1 docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/comprehensive_analysis_simple.py

# PowerShell
docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/comprehensive_analysis_simple.py
```

El an√°lisis completo generar√° **15 archivos CSV** con an√°lisis detallados.

- üìà **Estad√≠sticas globales** (media, mediana, varianza, desv. est√°ndar)
- üìä **Distribuci√≥n de ratings** (conteo y porcentaje por calificaci√≥n)
- üìÖ **An√°lisis temporal** (actividad por a√±o, mes y d√≠a de semana)
- üéÆ **Top juegos** (m√°s rese√±ados, mejor/peor valorados)
- **An√°lisis de texto** (palabras frecuentes positivas/negativas, longitud vs rating)
- üîç **Detecci√≥n de outliers** (rese√±as an√≥malas)
- ‚úì **Verificaci√≥n** (comparaci√≥n verified vs no verified)
- üëç **Helpful votes** (rese√±as m√°s √∫tiles)
- üë• **Top reviewers** (usuarios m√°s activos)

**Salida esperada del an√°lisis completo:**

```
üìä Leyendo datos desde HDFS...
‚úì Datos cargados: 2565349 rese√±as
1Ô∏è‚É£ Calculando estad√≠sticas descriptivas globales...
‚úì global_statistics.csv guardado exitosamente
2Ô∏è‚É£ Analizando distribuci√≥n de ratings...
‚úì rating_distribution.csv guardado exitosamente
3Ô∏è‚É£ An√°lisis temporal por a√±o...
‚úì yearly_activity.csv guardado exitosamente
4Ô∏è‚É£ An√°lisis temporal por mes...
‚úì monthly_activity.csv guardado exitosamente
5Ô∏è‚É£ Top 1000 juegos m√°s rese√±ados...
‚úì top_reviewed_games.csv guardado exitosamente
6Ô∏è‚É£ Top 1000 juegos mejor valorados...
‚úì top_rated_games.csv guardado exitosamente
7Ô∏è‚É£ Top 1000 juegos peor valorados...
‚úì worst_rated_games.csv guardado exitosamente
8Ô∏è‚É£ Correlaci√≥n longitud de texto vs rating...
‚úì length_vs_rating.csv guardado exitosamente
9Ô∏è‚É£ Palabras m√°s frecuentes en rese√±as positivas...
‚úì positive_words_frequency.csv guardado exitosamente
üîü Palabras m√°s frecuentes en rese√±as negativas...
‚úì negative_words_frequency.csv guardado exitosamente
1Ô∏è‚É£1Ô∏è‚É£ Detecci√≥n de outliers en ratings...
‚úì rating_outliers.csv guardado exitosamente
1Ô∏è‚É£2Ô∏è‚É£ Comparaci√≥n: Rese√±as verificadas vs no verificadas...
‚úì verified_statistics.csv guardado exitosamente
1Ô∏è‚É£3Ô∏è‚É£ An√°lisis por d√≠a de la semana...
‚úì day_of_week_analysis.csv guardado exitosamente
1Ô∏è‚É£4Ô∏è‚É£ An√°lisis de helpful votes...
‚úì helpful_votes_analysis.csv guardado exitosamente
1Ô∏è‚É£5Ô∏è‚É£ Top 1000 reviewers m√°s activos...
‚úì top_reviewers.csv guardado exitosamente

============================================================
‚úÖ An√°lisis completo finalizado!
üìÅ 15 archivos generados en /data/results/
============================================================
```

### Paso 9: Verificar los Resultados Generados

**Listar archivos generados:**

```bash
docker exec spark-master ls -lh /data/results/
```

**Ver contenido de un archivo espec√≠fico:**

```bash
# Estad√≠sticas globales
docker exec spark-master head -n 10 /data/results/global_statistics.csv

# Top 10 juegos m√°s rese√±ados
docker exec spark-master head -n 11 /data/results/top_reviewed_games.csv
```

### Paso 10: Probar la API

La API est√° disponible en **http://localhost:8000** con 17 endpoints organizados en 8 categor√≠as.

### Paso 10: Probar la API

La API est√° disponible en **http://localhost:8000** con 17 endpoints organizados en 8 categor√≠as.

**Documentaci√≥n Interactiva:**

- üìö **Swagger UI**: http://localhost:8000/docs
- üìñ **ReDoc**: http://localhost:8000/redoc

**Pruebas r√°pidas:**

```bash
# Informaci√≥n general de la API
curl http://localhost:8000/

# Estad√≠sticas globales
curl http://localhost:8000/statistics/global

# Top 10 juegos m√°s rese√±ados
curl "http://localhost:8000/games/top-reviewed?limit=10"
```

---

## üì° Endpoints de la API

### 1Ô∏è‚É£ General

| M√©todo | Endpoint | Descripci√≥n                                          |
| ------ | -------- | ---------------------------------------------------- |
| GET    | `/`      | Informaci√≥n general de la API y an√°lisis disponibles |
| GET    | `/docs`  | Documentaci√≥n interactiva (Swagger UI)               |
| GET    | `/redoc` | Documentaci√≥n alternativa (ReDoc)                    |

### 2Ô∏è‚É£ Estad√≠sticas (`/statistics`)

| M√©todo | Endpoint                          | Descripci√≥n                                                                       | Ejemplo                                                     |
| ------ | --------------------------------- | --------------------------------------------------------------------------------- | ----------------------------------------------------------- |
| GET    | `/statistics/global`              | Estad√≠sticas descriptivas globales (media, mediana, varianza, skewness, kurtosis) | `curl http://localhost:8000/statistics/global`              |
| GET    | `/statistics/rating-distribution` | Distribuci√≥n de ratings (conteo y porcentaje por rating 1-5)                      | `curl http://localhost:8000/statistics/rating-distribution` |
| GET    | `/statistics/verified`            | Comparaci√≥n entre rese√±as verificadas vs no verificadas                           | `curl http://localhost:8000/statistics/verified`            |

### 3Ô∏è‚É£ An√°lisis Temporal (`/temporal`)

| M√©todo | Endpoint                | Descripci√≥n                               | Ejemplo                                           |
| ------ | ----------------------- | ----------------------------------------- | ------------------------------------------------- |
| GET    | `/temporal/yearly`      | Actividad de rese√±as por a√±o              | `curl http://localhost:8000/temporal/yearly`      |
| GET    | `/temporal/monthly`     | Actividad de rese√±as por mes              | `curl http://localhost:8000/temporal/monthly`     |
| GET    | `/temporal/day-of-week` | Actividad de rese√±as por d√≠a de la semana | `curl http://localhost:8000/temporal/day-of-week` |

### 4Ô∏è‚É£ An√°lisis por Juego (`/games`)

| M√©todo | Endpoint              | Par√°metros             | Descripci√≥n                                  | Ejemplo                                                    |
| ------ | --------------------- | ---------------------- | -------------------------------------------- | ---------------------------------------------------------- |
| GET    | `/games/top-reviewed` | `limit` (default: 100) | Top juegos con m√°s rese√±as                   | `curl "http://localhost:8000/games/top-reviewed?limit=10"` |
| GET    | `/games/top-rated`    | `limit` (default: 100) | Top juegos mejor valorados (m√≠n. 10 rese√±as) | `curl "http://localhost:8000/games/top-rated?limit=10"`    |
| GET    | `/games/worst-rated`  | `limit` (default: 100) | Juegos peor valorados (m√≠n. 10 rese√±as)      | `curl "http://localhost:8000/games/worst-rated?limit=10"`  |

### 5Ô∏è‚É£ An√°lisis de Texto (`/text`)

| M√©todo | Endpoint                 | Par√°metros             | Descripci√≥n                                               | Ejemplo                                                     |
| ------ | ------------------------ | ---------------------- | --------------------------------------------------------- | ----------------------------------------------------------- |
| GET    | `/text/length-vs-rating` | -                      | Relaci√≥n entre longitud de rese√±a y rating                | `curl http://localhost:8000/text/length-vs-rating`          |
| GET    | `/text/positive-words`   | `limit` (default: 100) | Palabras m√°s frecuentes en rese√±as positivas (rating ‚â• 4) | `curl "http://localhost:8000/text/positive-words?limit=20"` |
| GET    | `/text/negative-words`   | `limit` (default: 100) | Palabras m√°s frecuentes en rese√±as negativas (rating ‚â§ 2) | `curl "http://localhost:8000/text/negative-words?limit=20"` |
| GET    | `/text/helpful-votes`    | `limit` (default: 100) | Rese√±as con m√°s votos de utilidad                         | `curl "http://localhost:8000/text/helpful-votes?limit=10"`  |

### 6Ô∏è‚É£ An√°lisis de Usuarios (`/users`)

| M√©todo | Endpoint               | Par√°metros             | Descripci√≥n                                 | Ejemplo                                                     |
| ------ | ---------------------- | ---------------------- | ------------------------------------------- | ----------------------------------------------------------- |
| GET    | `/users/top-reviewers` | `limit` (default: 100) | Usuarios m√°s activos (m√°s rese√±as escritas) | `curl "http://localhost:8000/users/top-reviewers?limit=10"` |

### 7Ô∏è‚É£ Detecci√≥n de Anomal√≠as (`/outliers`)

| M√©todo | Endpoint            | Par√°metros             | Descripci√≥n                                             | Ejemplo                                                  |
| ------ | ------------------- | ---------------------- | ------------------------------------------------------- | -------------------------------------------------------- |
| GET    | `/outliers/ratings` | `limit` (default: 100) | Rese√±as con ratings muy alejados del promedio del juego | `curl "http://localhost:8000/outliers/ratings?limit=50"` |

### 8Ô∏è‚É£ Productos con Nombres (`/products`) üÜï

**Requiere configuraci√≥n de API Key de Easyparser** (ver [Paso 2](#paso-2-configurar-la-api-key-de-easyparser))

| M√©todo | Endpoint                       | Par√°metros           | Descripci√≥n                                           | Ejemplo                                                            |
| ------ | ------------------------------ | -------------------- | ----------------------------------------------------- | ------------------------------------------------------------------ |
| GET    | `/products/top-reviewed-names` | `limit` (default: 5) | Top 5 productos m√°s rese√±ados con nombres de Amazon   | `curl "http://localhost:8000/products/top-reviewed-names?limit=5"` |
| GET    | `/products/top-rated-names`    | `limit` (default: 5) | Top 5 productos mejor valorados con nombres de Amazon | `curl "http://localhost:8000/products/top-rated-names?limit=5"`    |
| GET    | `/products/all-with-names`     | -                    | Ambos: Top reviewed y top rated con nombres           | `curl http://localhost:8000/products/all-with-names`               |

**Caracter√≠sticas de los endpoints de productos:**

- ‚úÖ **Cach√© Inteligente**: Solo consulta Easyparser API si el JSON no existe
- ‚úÖ **Sin l√≠mite de consultas**: Reutiliza datos cacheados
- ‚úÖ **Informaci√≥n completa**: ASIN, nombre, URL, estado
- üìÅ **Archivos de cach√©**: `/data/results/top_reviewed_with_names.json`, `/data/results/top_rated_with_names.json`

**Ejemplo de respuesta:**

```json
{
  "total": 5,
  "products": [
    {
      "asin": "B00JJNQG98",
      "product_name": "HyperX Cloud Gaming Headset",
      "url": "https://www.amazon.com/dp/B00JJNQG98",
      "status": "success"
    }
  ],
  "cached": true,
  "note": "Los nombres se obtienen de Easyparser API solo si no existe el cach√©"
}
```

### 9Ô∏è‚É£ Legacy (`/stats`)

| M√©todo | Endpoint | Descripci√≥n                                                           | Ejemplo                            |
| ------ | -------- | --------------------------------------------------------------------- | ---------------------------------- |
| GET    | `/stats` | **[LEGACY]** Endpoint antiguo - usa `/games/top-reviewed` en su lugar | `curl http://localhost:8000/stats` |

---

## üìã Ejemplos de Uso de la API

### Ejemplo 1: Obtener Estad√≠sticas Globales

```bash
curl http://localhost:8000/statistics/global
```

### Ejemplo 1: Obtener Estad√≠sticas Globales

```bash
curl http://localhost:8000/statistics/global
```

**Respuesta:**

```json
{
  "mean_rating": 4.156,
  "stddev_rating": 1.234,
  "variance_rating": 1.523,
  "median_rating": 5.0,
  "total_reviews": 2565349,
  "avg_review_length": 287.5,
  "avg_word_count": 52.3
}
```

### Ejemplo 2: Top 5 Juegos M√°s Rese√±ados

```bash
curl "http://localhost:8000/games/top-reviewed?limit=5"
```

**Respuesta:**

```json
[
  {
    "asin": "B00178630A",
    "review_count": 15683,
    "avg_rating": 4.2
  },
  ...
]
```

### Ejemplo 3: Obtener Nombres de Productos desde Amazon

```bash
curl "http://localhost:8000/products/top-reviewed-names?limit=3"
```

**Respuesta:**

```json
{
  "total": 3,
  "products": [
    {
      "asin": "B00JJNQG98",
      "product_name": "HyperX Cloud Gaming Headset for PC & PS4",
      "url": "https://www.amazon.com/dp/B00JJNQG98",
      "status": "success"
    }
  ],
  "cached": true
}
```

### Ejemplo 4: An√°lisis Temporal por A√±o

```bash
curl http://localhost:8000/temporal/yearly
```

### Ejemplo 5: Palabras Positivas M√°s Frecuentes

```bash
curl "http://localhost:8000/text/positive-words?limit=10"
```

---

## üîß Comandos √ötiles para Administraci√≥n

## üîß Comandos √ötiles para Administraci√≥n

### Gesti√≥n de Contenedores

```bash
# Ver logs de un contenedor espec√≠fico
docker logs spark-master --follow
docker logs api --follow
docker logs namenode --tail 50

# Reiniciar un servicio espec√≠fico
docker-compose restart api
docker-compose restart spark-master

# Reiniciar todos los servicios
docker-compose restart

# Detener todos los servicios
docker-compose down

# Detener y eliminar vol√∫menes (‚ö†Ô∏è elimina todos los datos)
docker-compose down -v

# Reconstruir y reiniciar la API (despu√©s de cambios en c√≥digo)
docker-compose build api && docker-compose up -d api
```

### Acceso a Contenedores

```bash
# Acceder al shell de un contenedor
docker exec -it spark-master bash
docker exec -it namenode bash
docker exec -it api bash

# Ejecutar comando en contenedor
docker exec spark-master ls -la /data/results/
docker exec api cat /app/amazon_scraper.py
```

### Gesti√≥n de HDFS

```bash
# Ver estado del cluster HDFS
docker exec namenode hdfs dfsadmin -report

# Listar archivos en HDFS
docker exec namenode hdfs dfs -ls /videogames/

# Salir del modo seguro (si es necesario)
docker exec namenode hdfs dfsadmin -safemode leave

# Ver espacio usado en HDFS
docker exec namenode hdfs dfs -df -h

# Eliminar archivos de HDFS
docker exec namenode hdfs dfs -rm /videogames/Video_Games.json
```

### Gesti√≥n de Cach√© de Productos

```bash
# Listar archivos de cach√©
docker exec api ls -lh /data/results/*.json

# Eliminar cach√© para regenerar (fuerza nueva consulta a Easyparser)
docker exec api rm /data/results/top_reviewed_with_names.json
docker exec api rm /data/results/top_rated_with_names.json

# Ver contenido del cach√©
docker exec api cat /data/results/top_reviewed_with_names.json
```

### Monitoreo y Debugging

```bash
# Ver uso de recursos de contenedores
docker stats

# Inspeccionar un contenedor
docker inspect api

# Ver puertos expuestos
docker port api

# Verificar conectividad entre contenedores
docker exec api ping spark-master
docker exec spark-master ping namenode
```

## üìä Endpoints de la API

| M√©todo | Endpoint | Descripci√≥n                                                 |
| ------ | -------- | ----------------------------------------------------------- |
| GET    | `/`      | Mensaje de bienvenida                                       |
| GET    | `/stats` | Estad√≠sticas de videojuegos (asin, avg_score, review_count) |
| GET    | `/docs`  | Documentaci√≥n interactiva Swagger UI                        |
| GET    | `/redoc` | Documentaci√≥n alternativa ReDoc                             |

## üêõ Soluci√≥n de Problemas

### Error: "Cannot connect to Docker daemon"

**Problema**: Docker Desktop no est√° ejecut√°ndose.

```bash
# Verificar versi√≥n de Docker
docker --version

# En Windows, aseg√∫rate de que Docker Desktop est√© corriendo
```

**Soluci√≥n**: Inicia Docker Desktop y espera a que est√© completamente cargado.

---

### Error: "port is already allocated"

**Problema**: Los puertos 8000, 8080, 9000 o 9870 ya est√°n en uso.

```bash
# Detener contenedores que usen los puertos
docker-compose down

# Verificar qu√© proceso usa un puerto (Windows)
netstat -ano | findstr :8000

# Matar proceso por PID (reemplaza 1234 con el PID real)
taskkill /PID 1234 /F
```

**Soluci√≥n Alternativa**: Cambiar los puertos en `docker-compose.yml`:

```yaml
api:
  ports:
    - '8001:8000' # Cambiar puerto host de 8000 a 8001
```

---

### Error: "No such file or directory" al ejecutar spark-submit

**Problema**: Git Bash en Windows convierte rutas autom√°ticamente.

```bash
# ‚ùå Incorrecto (Git Bash)
docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/spark_analysis.py

# ‚úÖ Correcto (Git Bash)
MSYS_NO_PATHCONV=1 docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/spark_analysis.py

# ‚úÖ Correcto (PowerShell)
docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/spark_analysis.py
```

---

### Error: "Analysis not found" en la API

**Problema**: Los archivos CSV no han sido generados por Spark.

```bash
# Verificar si existen los archivos
docker exec spark-master ls -la /data/results/

# Si el directorio est√° vac√≠o, ejecutar el an√°lisis
MSYS_NO_PATHCONV=1 docker exec spark-master /spark/bin/spark-submit /opt/spark-apps/comprehensive_analysis_simple.py
```

---

### Error: HDFS est√° en "Safe Mode"

**Problema**: HDFS no permite escrituras porque est√° en modo seguro.

```bash
# Verificar estado de HDFS
docker exec namenode hdfs dfsadmin -safemode get

# Salir del modo seguro
docker exec namenode hdfs dfsadmin -safemode leave
```

---

### Error: "Easyparser API authentication failed"

**Problema**: La API Key de Easyparser no est√° configurada o es inv√°lida.

**Soluci√≥n**:

1. Verifica que configuraste tu API Key en `api/amazon_scraper.py` (l√≠nea 15)
2. Reconstruye el contenedor de la API:

```bash
docker-compose build api && docker-compose up -d api
```

3. Verifica que la API Key sea correcta consultando el dashboard de Easyparser

---

### Error: "Connection timeout" al subir archivo a HDFS

**Problema**: El contenedor namenode no est√° listo o hay problemas de red.

**Soluci√≥n**:

```bash
# Esperar 60 segundos y reintentar
sleep 60

# Verificar que namenode est√° corriendo
docker ps | grep namenode

# Verificar logs de namenode
docker logs namenode --tail 50

# Reiniciar namenode si es necesario
docker-compose restart namenode
```

---

### Productos devuelven "N/A - T√≠tulo no encontrado"

**Problema**: Algunos ASINs son antiguos o no est√°n disponibles en Amazon.

**Explicaci√≥n**: Esto es **normal**. Algunos productos del dataset son de 2014-2018 y pueden:

- Ya no estar disponibles en Amazon
- Tener p√°ginas desactivadas
- Ser regionales (solo disponibles en ciertos pa√≠ses)

**Soluci√≥n**: Los productos con `"status": "success"` tienen informaci√≥n v√°lida. Usa esos para tus an√°lisis.

---

### API lenta en primera consulta de productos

**Problema**: La primera consulta a Easyparser puede tardar 10-30 segundos.

**Explicaci√≥n**: Esto es **normal** porque:

- Se consulta la API externa de Easyparser
- Se procesan 5 productos en secuencia
- Se guarda el cach√© en JSON

**Soluci√≥n**: Las consultas subsecuentes son instant√°neas gracias al cach√©.

---

### Contenedores se reinician constantemente

**Problema**: Falta de recursos (RAM/CPU).

```bash
# Ver uso de recursos
docker stats

# Verificar logs de error
docker logs namenode
docker logs spark-master
```

**Soluci√≥n**:

- Asigna m√°s recursos a Docker Desktop (m√≠nimo 8GB RAM)
- Cierra aplicaciones que consuman mucha memoria
- En `docker-compose.yml` reduce workers de Spark

---

## üìÅ Estructura del Proyecto

```
video-game-bigdata/
‚îú‚îÄ‚îÄ docker-compose.yml              # Configuraci√≥n de servicios Docker
‚îú‚îÄ‚îÄ README.md                       # Esta gu√≠a completa
‚îÇ
‚îú‚îÄ‚îÄ api/                            # API REST (FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ dockerfile                  # Imagen Docker de la API
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # C√≥digo principal de la API (17 endpoints)
‚îÇ   ‚îú‚îÄ‚îÄ amazon_scraper.py           # ‚≠ê Integraci√≥n con Easyparser API
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Dependencias Python (fastapi, pandas, requests)
‚îÇ   ‚îî‚îÄ‚îÄ DOCUMENTACION_PRODUCTOS.md  # Documentaci√≥n de endpoints de productos
‚îÇ
‚îú‚îÄ‚îÄ data/                           # Datos de origen
‚îÇ   ‚îî‚îÄ‚îÄ Video_Games.json            # Dataset de rese√±as (1.7GB, 2.5M rese√±as)
‚îÇ
‚îú‚îÄ‚îÄ hadoop/                         # Configuraci√≥n de Hadoop/HDFS
‚îÇ   ‚îú‚îÄ‚îÄ core-site.xml               # Configuraci√≥n de Hadoop Core
‚îÇ   ‚îî‚îÄ‚îÄ hdfs-site.xml               # Configuraci√≥n de HDFS
‚îÇ
‚îî‚îÄ‚îÄ spark/                          # Scripts de an√°lisis con PySpark
    ‚îú‚îÄ‚îÄ comprehensive_analysis_simple.py  # An√°lisis completo (15 CSV)
    ‚îî‚îÄ‚îÄ spark_analysis.py                 # An√°lisis b√°sico (1 CSV)
```

### Archivos Generados (dentro del volumen Docker)

```
/data/results/                      # Resultados del an√°lisis de Spark
‚îú‚îÄ‚îÄ global_statistics.csv           # Estad√≠sticas globales
‚îú‚îÄ‚îÄ rating_distribution.csv         # Distribuci√≥n de ratings
‚îú‚îÄ‚îÄ yearly_activity.csv             # Actividad por a√±o
‚îú‚îÄ‚îÄ monthly_activity.csv            # Actividad por mes
‚îú‚îÄ‚îÄ day_of_week_analysis.csv        # Actividad por d√≠a
‚îú‚îÄ‚îÄ top_reviewed_games.csv          # Top juegos m√°s rese√±ados
‚îú‚îÄ‚îÄ top_rated_games.csv             # Top juegos mejor valorados
‚îú‚îÄ‚îÄ worst_rated_games.csv           # Juegos peor valorados
‚îú‚îÄ‚îÄ length_vs_rating.csv            # Longitud vs rating
‚îú‚îÄ‚îÄ positive_words_frequency.csv    # Palabras positivas
‚îú‚îÄ‚îÄ negative_words_frequency.csv    # Palabras negativas
‚îú‚îÄ‚îÄ rating_outliers.csv             # Outliers detectados
‚îú‚îÄ‚îÄ verified_statistics.csv         # Rese√±as verificadas
‚îú‚îÄ‚îÄ helpful_votes_analysis.csv      # Rese√±as m√°s √∫tiles
‚îú‚îÄ‚îÄ top_reviewers.csv               # Usuarios m√°s activos
‚îú‚îÄ‚îÄ top_reviewed_with_names.json    # üîÑ Cach√© de productos (top reviewed)
‚îî‚îÄ‚îÄ top_rated_with_names.json       # üîÑ Cach√© de productos (top rated)
```

## üîç Detalles T√©cnicos

### Procesamiento de Datos con Spark

**Script**: `comprehensive_analysis_simple.py`

**Pipeline de procesamiento**:

1. **Lectura**: Lee el JSON desde HDFS (`hdfs://namenode:9000/videogames/Video_Games.json`)
2. **Limpieza**: Filtra registros con valores nulos, calcula m√©tricas derivadas
3. **Transformaciones**:
   - Extrae a√±o, mes, d√≠a de semana de las fechas
   - Calcula longitud de texto y conteo de palabras
   - Tokeniza y analiza texto con ML
4. **Agregaciones**: Calcula promedios, conteos, distribuciones por m√∫ltiples dimensiones
5. **Escritura**: Genera 15 archivos CSV en el volumen compartido

**T√©cnicas avanzadas utilizadas**:

- Window Functions para an√°lisis de outliers
- TF-IDF para an√°lisis de palabras frecuentes
- Percentiles y cuartiles para distribuciones
- Detecci√≥n de anomal√≠as con desviaci√≥n est√°ndar

### Arquitectura de la API

**Framework**: FastAPI 0.104.1

**Caracter√≠sticas**:

- 17 endpoints REST organizados en 8 categor√≠as
- Validaci√≥n autom√°tica de par√°metros con Pydantic
- Documentaci√≥n interactiva con Swagger UI y ReDoc
- CORS habilitado para uso desde frontends
- Cach√© inteligente para consultas a APIs externas
- Manejo robusto de errores con c√≥digos HTTP apropiados

**Integraci√≥n con Easyparser**:

- Sistema de cach√© basado en archivos JSON
- Consulta bajo demanda (solo si no existe cach√©)
- Manejo de errores HTTP, timeouts y productos no encontrados
- Estructura de respuesta enriquecida con metadatos

### Vol√∫menes de Docker

| Volumen       | Tipo         | Uso                         | Persistencia   |
| ------------- | ------------ | --------------------------- | -------------- |
| `namenode`    | Named volume | Metadatos de HDFS           | ‚úÖ Persistente |
| `datanode`    | Named volume | Datos de HDFS (1.7GB)       | ‚úÖ Persistente |
| `shared-data` | Named volume | Resultados CSV y cach√© JSON | ‚úÖ Persistente |

**Ventajas del volumen compartido**:

- ‚úÖ Los datos sobreviven a reinicios de contenedores
- ‚úÖ Permite compartir resultados entre Spark y API
- ‚úÖ No requiere acceso desde el host
- ‚úÖ Mejor rendimiento que bind mounts

### Red de Docker

**Tipo**: Bridge network (`hadoop`)

**Comunicaci√≥n entre contenedores**:

- `spark-master` ‚Üí `namenode:9000` (lectura HDFS)
- `api` ‚Üí `shared-data:/data/results` (lectura CSV)
- `api` ‚Üí `https://realtime.easyparser.com` (consulta externa)

### Dataset

**Fuente**: Amazon Customer Reviews (Video Games)  
**Tama√±o**: 1.7GB comprimido  
**Registros**: 2,565,349 rese√±as  
**Periodo**: 1996-2018  
**Campos principales**:

- `asin`: Identificador √∫nico del producto
- `reviewerID`: ID del usuario que escribi√≥ la rese√±a
- `overall`: Rating (1-5 estrellas)
- `reviewText`: Texto de la rese√±a
- `summary`: Resumen de la rese√±a
- `unixReviewTime`: Timestamp Unix
- `verified`: Si la compra fue verificada
- `helpful`: Votos de utilidad [√∫tiles, totales]

## üìà Pr√≥ximos Pasos y Mejoras

### Funcionalidades Planificadas

- [ ] **Variables de entorno**: Externalizar API Key de Easyparser a `.env`
- [ ] **Filtros avanzados**: B√∫squeda por rango de fechas, rating, plataforma
- [ ] **An√°lisis de sentimiento**: Clasificaci√≥n autom√°tica de rese√±as (positivo/negativo/neutral)
- [ ] **Gr√°ficos y visualizaciones**: Endpoint para generar gr√°ficos con matplotlib/plotly
- [ ] **Recomendaciones**: Sistema de recomendaci√≥n basado en similitud de rese√±as
- [ ] **Cach√© Redis**: Reemplazar cach√© en archivo por Redis para mejor rendimiento
- [ ] **Autenticaci√≥n**: JWT tokens para proteger endpoints
- [ ] **Rate limiting**: Limitar n√∫mero de peticiones por IP
- [ ] **Webhooks**: Notificaciones cuando se complete el an√°lisis de Spark
- [ ] **Paginaci√≥n**: Soporte para grandes resultados con offset/limit
- [ ] **Export formats**: Permitir descargar resultados en CSV, Excel, JSON

### Mejoras T√©cnicas

- [ ] **Tests automatizados**: Pytest para API y Spark jobs
- [ ] **CI/CD**: GitHub Actions para despliegue autom√°tico
- [ ] **Logging estructurado**: ELK Stack (Elasticsearch, Logstash, Kibana)
- [ ] **Monitoreo**: Prometheus + Grafana para m√©tricas en tiempo real
- [ ] **Optimizaci√≥n Spark**: Particionamiento y caching estrat√©gico
- [ ] **Compresi√≥n**: Usar Parquet en lugar de CSV para mejor rendimiento
- [ ] **Spark Streaming**: An√°lisis en tiempo real de nuevas rese√±as
- [ ] **Multi-idioma**: Soporte para an√°lisis en espa√±ol, franc√©s, etc.

### Despliegue en Producci√≥n

- [ ] **Kubernetes**: Orquestar contenedores con K8s
- [ ] **Cloud deployment**: AWS EMR, Azure HDInsight, Google Dataproc
- [ ] **Load balancing**: Nginx para distribuir tr√°fico de la API
- [ ] **HTTPS**: Certificados SSL con Let's Encrypt
- [ ] **CDN**: CloudFlare para cach√© de respuestas est√°ticas
- [ ] **Database**: PostgreSQL para almacenar resultados procesados
- [ ] **Backup autom√°tico**: Respaldos diarios de HDFS y resultados

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Para contribuir:

1. Fork el repositorio
2. Crea una rama con tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

### √Åreas que Necesitan Ayuda

- ÔøΩ Reportar bugs y problemas
- üìù Mejorar documentaci√≥n
- üß™ Escribir tests
- üé® Crear visualizaciones
- üåê Traducir a otros idiomas
- ‚ö° Optimizar rendimiento de Spark

## üìö Recursos Adicionales

### Documentaci√≥n Oficial

- [Apache Spark](https://spark.apache.org/docs/latest/)
- [Apache Hadoop](https://hadoop.apache.org/docs/stable/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Docker](https://docs.docker.com/)
- [Easyparser API](https://easyparser.com/docs)

### Tutoriales Relacionados

- [PySpark Tutorial](https://spark.apache.org/docs/latest/api/python/)
- [HDFS Commands](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/tutorial/)

### Dataset Original

- [Amazon Customer Reviews Dataset](https://nijianmo.github.io/amazon/index.html)

## ‚ùì Preguntas Frecuentes (FAQ)

### ¬øCu√°nto tiempo tarda el an√°lisis completo?

Entre 5-10 minutos dependiendo de tu hardware. Con 8GB RAM y CPU moderna, aproximadamente 6-7 minutos.

### ¬øPuedo usar mi propio dataset?

S√≠, solo necesitas:

1. Convertir tu dataset a JSON
2. Copiar el archivo a HDFS
3. Modificar `comprehensive_analysis_simple.py` para adaptarlo a tu esquema

### ¬øLos datos se pierden al reiniciar Docker?

No, los vol√∫menes de Docker (`namenode`, `datanode`, `shared-data`) son **persistentes**. Los datos sobreviven a reinicios. Solo se pierden si ejecutas `docker-compose down -v`.

### ¬øNecesito una API Key de Easyparser?

Solo si quieres usar los endpoints `/products/*` que obtienen nombres de Amazon. Los otros 14 endpoints funcionan sin API Key.

### ¬øCu√°ntas consultas tengo con la API gratuita de Easyparser?

Consulta el plan gratuito de Easyparser en su [p√°gina de precios](https://easyparser.com/pricing). El sistema de cach√© minimiza las consultas necesarias.

### ¬øPuedo escalar a m√°s workers de Spark?

S√≠, edita `docker-compose.yml` y agrega m√°s servicios `spark-worker-2`, `spark-worker-3`, etc.

### ¬øFunciona en Mac/Linux?

S√≠, el proyecto es multiplataforma. En Linux/Mac no necesitas `MSYS_NO_PATHCONV=1`.

## üêû Reporte de Bugs

Si encuentras un bug, por favor [abre un issue](https://github.com/StalinAM/video-game-bigdata/issues) con:

- Descripci√≥n del problema
- Pasos para reproducir
- Logs relevantes (`docker logs`)
- Sistema operativo y versi√≥n de Docker

## ÔøΩüë§ Autor

**Stalin Andrade**

- GitHub: [@StalinAM](https://github.com/StalinAM)
- Email: [tu-email@example.com]
- LinkedIn: [Tu perfil de LinkedIn]

## üìù Licencia

Este proyecto es de c√≥digo abierto y est√° disponible bajo la **Licencia MIT**.

```
MIT License

Copyright (c) 2026 Stalin Andrade

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ‚≠ê Dale una Estrella

Si este proyecto te result√≥ √∫til, considera darle una ‚≠ê en GitHub. ¬°Gracias!

---

**¬øPreguntas o problemas?** Abre un [issue en GitHub](https://github.com/StalinAM/video-game-bigdata/issues) o consulta la [documentaci√≥n completa](http://localhost:8000/docs).
